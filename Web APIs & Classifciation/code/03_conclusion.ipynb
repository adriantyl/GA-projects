{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs & Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Assembly DSI 19 Project 3 Adrian Teng "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, it is splitted into three parts respectively:\n",
    "- API & EDA (01_api_eda)\n",
    "- Processing & Modeling (02_processing_modeling)\n",
    "- Conclusion (03_conclusion)\n",
    "\n",
    "In the third notebook, 03_conclusion, unseen data was used to test against the selected models and further evaluted with the distribution of True positive, False Positive, False Negative and True Negative Preidtions. Additional features like, Accuaracy, Sensitivity, Specificity and Precision also put into actions.\n",
    "\n",
    "Feature Analysis was done to support problem statement in the conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "- Model 1: CV MB\n",
    "- Model 2: TFIDF LR\n",
    "- Feature Anaylsis\n",
    "- Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# library imports\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "\n",
    "\n",
    "# preprocessing imports\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/tifucofns.csv')\n",
    "df_pre = pd.read_csv('../datasets/df_pre.csv')\n",
    "X_train = pd.read_csv('../datasets/X_train.csv', index_col = 0)\n",
    "X_test = pd.read_csv('../datasets/X_test.csv', index_col = 0)\n",
    "y_train = pd.read_csv('../datasets/y_train.csv', index_col = 0)\n",
    "y_test = pd.read_csv('../datasets/y_test.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "post_stem    1\n",
       "post_lem     1\n",
       "tifu         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre.dropna(inplace = True) #remove nan values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: CountVectorizor MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_steps = [('m1_cv',CountVectorizer(stop_words='english', ngram_range=(1,1))),\n",
    "           ('m1_mnb',MultinomialNB())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_1 = Pipeline(m1_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('m1_cv', CountVectorizer(stop_words='english')),\n",
       "                ('m1_mnb', MultinomialNB())])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_1.fit(X_train.post_lem, y_train.tifu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9711711711711711"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train score\n",
    "pipe_1.score(X_train.post_lem, y_train.tifu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8652291105121294"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test score\n",
    "pipe_1.score(X_test.post_lem, y_test.tifu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9446320054017556"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unseen test score\n",
    "pipe_1.score(df_pre.post_lem, df_pre.tifu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 839\n",
      "False Positives: 47\n",
      "False Negatives: 35\n",
      "True Positives: 560\n",
      "\n",
      "Accuracy:  0.9446320054017556\n",
      "Sensitivity:  0.9411764705882353\n",
      "Specificity:  0.9469525959367946\n",
      "Precision:  0.9225700164744646\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(df_pre.tifu, pipe_1.predict(df_pre.post_lem)).ravel()\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "\n",
    "print(\"\\nAccuracy: \", (tn + tp) / (tn + fp + fn + tp))\n",
    "print(\"Sensitivity: \", tp / (tp + fn))\n",
    "print(\"Specificity: \", tn / (tn + fp))\n",
    "print(\"Precision: \", tp / (tp + fp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1110x13829 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 94359 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = pipe_1.named_steps['m1_mnb']\n",
    "cv1 = pipe_1.named_steps['m1_cv']\n",
    "cv1.fit_transform(X_train.post_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: TFIDFVectorizor Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_steps = [('m2_tf',TfidfVectorizer(stop_words='english', ngram_range=(1,1), max_features=30000)),\n",
    "            ('m2_ss',StandardScaler(with_mean=False)),\n",
    "            ('m2_lr',LogisticRegression())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_2 = Pipeline(m2_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('m2_tf',\n",
       "                 TfidfVectorizer(max_features=30000, stop_words='english')),\n",
       "                ('m2_ss', StandardScaler(with_mean=False)),\n",
       "                ('m2_lr', LogisticRegression())])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_2.fit(X_train.post_lem, y_train.tifu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train score\n",
    "pipe_2.score(X_train.post_lem, y_train.tifu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8867924528301887"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test score\n",
    "pipe_2.score(X_test.post_lem, y_test.tifu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9716407832545577"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unseen data score\n",
    "pipe_2.score(df_pre.post_lem, df_pre.tifu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 871\n",
      "False Positives: 15\n",
      "False Negatives: 27\n",
      "True Positives: 568\n",
      "\n",
      "Accuracy:  0.9716407832545577\n",
      "Sensitivity:  0.9546218487394958\n",
      "Specificity:  0.9830699774266366\n",
      "Precision:  0.9742710120068611\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(df_pre.tifu, pipe_2.predict(df_pre.post_lem)).ravel()\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "\n",
    "print(\"\\nAccuracy: \", (tn + tp) / (tn + fp + fn + tp))\n",
    "print(\"Sensitivity: \", tp / (tp + fn))\n",
    "print(\"Specificity: \", tn / (tn + fp))\n",
    "print(\"Precision: \", tp / (tp + fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = pipe_2.named_steps['m2_lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf2 = pipe_2.named_steps['m2_tf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1110x13829 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 94359 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf2.fit_transform(X_train.post_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_df = pd.DataFrame(m1.coef_.T, index=cv1.get_feature_names(), columns=['coef'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "just       -4.704559\n",
       "like       -4.996557\n",
       "time       -5.224905\n",
       "didn       -5.273912\n",
       "got        -5.297008\n",
       "dr         -5.486834\n",
       "tl         -5.501059\n",
       "went       -5.547989\n",
       "really     -5.569234\n",
       "know       -5.581580\n",
       "said       -5.739360\n",
       "going      -5.761420\n",
       "ve         -5.795450\n",
       "thought    -5.810954\n",
       "day        -5.822742\n",
       "don        -5.842703\n",
       "happened   -5.863070\n",
       "car        -5.913714\n",
       "today      -5.931181\n",
       "did        -5.990155\n",
       "Name: coef, dtype: float64"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1_df.coef.sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_df = pd.DataFrame(m2.coef_.T, index=tf2.get_feature_names(), columns=['coef'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tl              0.355900\n",
       "dr              0.342180\n",
       "tldr            0.173506\n",
       "hour            0.105832\n",
       "happened        0.096826\n",
       "today           0.095596\n",
       "minutes         0.092480\n",
       "obligatory      0.089691\n",
       "forgot          0.084122\n",
       "decided         0.081912\n",
       "morning         0.074589\n",
       "went            0.073344\n",
       "sitting         0.069247\n",
       "prank           0.069129\n",
       "forgetting      0.068725\n",
       "got             0.068466\n",
       "immediately     0.067605\n",
       "sleep           0.067374\n",
       "accidentally    0.066832\n",
       "starts          0.066280\n",
       "Name: coef, dtype: float64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2_df.coef.sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to count the number of unique words\n",
    "def important_features(vectorizer,classifier,n=20):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words for r/confessions\\n\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\\n\")\n",
    "    print(\"Important words for r/tifu\\n\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words for r/confessions\n",
      "\n",
      "0 864.0 just\n",
      "0 775.0 like\n",
      "0 589.0 don\n",
      "0 508.0 know\n",
      "0 476.0 feel\n",
      "0 412.0 want\n",
      "0 410.0 really\n",
      "0 402.0 ve\n",
      "0 398.0 time\n",
      "0 361.0 people\n",
      "0 296.0 life\n",
      "0 295.0 think\n",
      "0 279.0 didn\n",
      "0 265.0 got\n",
      "0 237.0 friends\n",
      "0 233.0 years\n",
      "0 215.0 day\n",
      "0 207.0 did\n",
      "0 206.0 things\n",
      "0 201.0 said\n",
      "-----------------------------------------\n",
      "\n",
      "Important words for r/tifu\n",
      "\n",
      "1 773.0 just\n",
      "1 577.0 like\n",
      "1 459.0 time\n",
      "1 437.0 didn\n",
      "1 427.0 got\n",
      "1 353.0 dr\n",
      "1 348.0 tl\n",
      "1 332.0 went\n",
      "1 325.0 really\n",
      "1 321.0 know\n",
      "1 274.0 said\n",
      "1 268.0 going\n",
      "1 259.0 ve\n",
      "1 255.0 thought\n",
      "1 252.0 day\n",
      "1 247.0 don\n",
      "1 242.0 happened\n",
      "1 230.0 car\n",
      "1 226.0 today\n",
      "1 213.0 did\n"
     ]
    }
   ],
   "source": [
    "#print the top 20 of each subredits\n",
    "important_features(pipe_1.named_steps['m1_cv'], pipe_1.named_steps['m1_mnb'], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. Model 2 (TFIDF Logistic Regression) have the highest test score for both test and unseen test, as compared to the other models as a sucess metric in comparing text of two similar subreddits.\n",
    "\n",
    "- 2. The similarity of the two subreddits is very high, from the feature analysis we can see that the top 20 words that overlapped is more than 70%.\n",
    "\n",
    "- 3. Other than the test score, the overall Sensistivity, Specificity, Precicion are higher than Model 1(CV MultinomialNB).\n",
    "\n",
    "- 4. Hence, if the post is so similar, Reddit user can consider posting in 'TIFU' instead of 'Confessions' to attract more viewers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
